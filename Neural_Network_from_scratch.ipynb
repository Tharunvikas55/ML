{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkVWMQ7QSPfICvb1eyA6pu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharunvikas55/ML/blob/main/Neural_Network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AxYJ03qGRq-1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrjdpVtxSpcy",
        "outputId": "4fe2d6dc-970a-4fce-9d8b-b710f748b0ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0],\n",
              "       [1, 0, 1, 1],\n",
              "       [0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.array([[1],[0],[1]])\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdU334i2Sz3c",
        "outputId": "833cf118-f290-490f-926a-2c4e832368a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "CL71_b2ETO4V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_sigmoid(x):\n",
        "  return x*(1-x)"
      ],
      "metadata": {
        "id": "ulwjAHEuTnj8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=5000\n",
        "lr=0.1\n",
        "input_neurons=x.shape[1]\n",
        "hidden_neurons=3\n",
        "output_neurons=1"
      ],
      "metadata": {
        "id": "c67fd6p-T5YP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wh=np.random.uniform(size=(input_neurons,hidden_neurons))\n",
        "bh=np.random.uniform(size=(1,hidden_neurons))\n",
        "wo=np.random.uniform(size=(hidden_neurons,output_neurons))\n",
        "bo=np.random.uniform(size=(1,output_neurons))"
      ],
      "metadata": {
        "id": "1OhqbxM1UZr4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "  #forward\n",
        "  hidden_input=np.dot(x,wh)\n",
        "  hidden_input=hidden_input+bh\n",
        "  hidden_activation=sigmoid(hidden_input)\n",
        "  output_layer=np.dot(hidden_activation,wo)\n",
        "  output=sigmoid(output_layer)\n",
        "  #backward\n",
        "  E=y-output\n",
        "  slope_output=derivative_sigmoid(output)\n",
        "  slope_hidden=derivative_sigmoid(hidden_activation)\n",
        "  d_output=E*slope_output\n",
        "  error_hidden=d_output.dot(wo.T)\n",
        "  d_hidden=error_hidden*slope_hidden\n",
        "  wo+=hidden_activation.T.dot(d_output)*lr\n",
        "  bo+=np.sum(d_output,axis=0,keepdims=True)*lr\n",
        "  wh+=x.T.dot(d_hidden)*lr\n",
        "  bh+=np.sum(d_hidden,axis=0,keepdims=True)*lr\n",
        ""
      ],
      "metadata": {
        "id": "42w21JFQVNLz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRTGstU6Y2Zp",
        "outputId": "e4a75fc8-c56d-424a-afa5-6a8e546e0f3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9483918 ],\n",
              "       [0.07358864],\n",
              "       [0.95682984]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iris Dataset"
      ],
      "metadata": {
        "id": "BJqExLp2bqHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# Preprocess the data\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('encoder', OneHotEncoder(), [0])  # One-hot encode the target variable\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "y = np.array(ct.fit_transform(y), dtype=np.float32)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_dim=4, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=5)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "hMvWHVQPY5Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# Preprocess the data\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('encoder', OneHotEncoder(), [0])  # One-hot encode the target variable\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "y = np.array(ct.fit_transform(y), dtype=np.float32)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to TensorFlow tensors\n",
        "X_train_tf = tf.constant(X_train, dtype=tf.float32)\n",
        "y_train_tf = tf.constant(y_train, dtype=tf.float32)\n",
        "X_test_tf = tf.constant(X_test, dtype=tf.float32)\n",
        "y_test_tf = tf.constant(y_test, dtype=tf.float32)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.weights = tf.Variable(tf.random.normal([4, 3], stddev=0.1), trainable=True)\n",
        "        self.biases = tf.Variable(tf.zeros([3]), trainable=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return tf.nn.softmax(tf.matmul(inputs, self.weights) + self.biases)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Initialize the model\n",
        "model = NeuralNetwork()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model.forward(X_train_tf)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_train_tf, predictions))\n",
        "\n",
        "    # Compute gradients and update weights\n",
        "    gradients = tape.gradient(loss, [model.weights, model.biases])\n",
        "    model.weights.assign_sub(learning_rate * gradients[0])\n",
        "    model.biases.assign_sub(learning_rate * gradients[1])\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy():.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_predictions = model.forward(X_test_tf)\n",
        "test_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_test_tf, test_predictions))\n",
        "accuracy = tf.reduce_mean(tf.keras.metrics.categorical_accuracy(y_test_tf, test_predictions))\n",
        "\n",
        "print(f'Test Loss: {test_loss.numpy():.4f}, Test Accuracy: {accuracy.numpy():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIM49BNQb3m7",
        "outputId": "76c07f5e-1f77-4d20-c23b-e2d1cf08ff37"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Loss: 0.6154\n",
            "Epoch 200/1000, Loss: 0.5149\n",
            "Epoch 300/1000, Loss: 0.4639\n",
            "Epoch 400/1000, Loss: 0.4311\n",
            "Epoch 500/1000, Loss: 0.4073\n",
            "Epoch 600/1000, Loss: 0.3886\n",
            "Epoch 700/1000, Loss: 0.3733\n",
            "Epoch 800/1000, Loss: 0.3602\n",
            "Epoch 900/1000, Loss: 0.3487\n",
            "Epoch 1000/1000, Loss: 0.3384\n",
            "Test Loss: 0.2839, Test Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to TensorFlow tensors\n",
        "X_train_tf = tf.constant(X_train, dtype=tf.float32)\n",
        "y_train_tf = tf.constant(y_train, dtype=tf.float32)\n",
        "X_test_tf = tf.constant(X_test, dtype=tf.float32)\n",
        "y_test_tf = tf.constant(y_test, dtype=tf.float32)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.weights = tf.Variable(tf.random.normal([X_train.shape[1], 1], stddev=0.1), trainable=True)\n",
        "        self.biases = tf.Variable(tf.zeros([1]), trainable=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return tf.nn.sigmoid(tf.matmul(inputs, self.weights) + self.biases)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Initialize the model\n",
        "model = NeuralNetwork()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model.forward(X_train_tf)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_train_tf, predictions))\n",
        "\n",
        "    # Compute gradients and update weights\n",
        "    gradients = tape.gradient(loss, [model.weights, model.biases])\n",
        "    model.weights.assign_sub(learning_rate * gradients[0])\n",
        "    model.biases.assign_sub(learning_rate * gradients[1])\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy():.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_predictions = model.forward(X_test_tf)\n",
        "test_predictions_binary = tf.where(test_predictions >= 0.5, 1.0, 0.0)\n",
        "accuracy = accuracy_score(y_test_tf, test_predictions_binary)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtZHavDPcSa9",
        "outputId": "a3973d54-f6e2-403c-ea79-24e281c002d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Loss: 0.2664\n",
            "Epoch 200/1000, Loss: 0.1983\n",
            "Epoch 300/1000, Loss: 0.1681\n",
            "Epoch 400/1000, Loss: 0.1502\n",
            "Epoch 500/1000, Loss: 0.1381\n",
            "Epoch 600/1000, Loss: 0.1292\n",
            "Epoch 700/1000, Loss: 0.1224\n",
            "Epoch 800/1000, Loss: 0.1169\n",
            "Epoch 900/1000, Loss: 0.1124\n",
            "Epoch 1000/1000, Loss: 0.1086\n",
            "Test Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAvIy6tgcoey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}